# -*- coding: utf-8 -*-
"""CMPE460Assignment1Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1blM-1a21BcHo4gf8PGF2l4T-VIzk139t

"""

# Necessary Libraries:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Activation function:
# Returns +1 if z >= 0, else returns -1:
def sign(z):
    return np.where(z >= 0, 1, -1)

# Perceptron model function:
# This function computes the weighted sum of inputs and applies the sign activation function.
# Parameters:
#   W: weight vector
#   x: input vector
# Returns:
#   +1 or -1 as the model output:
def perceptron(W, x):
    return sign(np.dot(W, x))

# Perceptron training function:
# This function trains the perceptron model using the input data and labels.
# Parameters:
#   X: Input features (2D array, each row is a sample)
#   Y: Target labels (+1 or -1 for each sample)
#   eta: Learning rate (default is 1)
#   epochs: Maximum number of training epochs
# Returns:
#   Trained weight vector including the bias
def perceptron_train(X, Y, eta=1, epochs=100):
    # Initializing weights to zero, including bias term:
    W = np.zeros(X.shape[1] + 1)

    # Bias feature (1) to each input sample in X:
    X_bias = np.c_[np.ones(X.shape[0]), X]

    # Training loop over the specified number of epochs:
    for epoch in range(epochs):
        errors = 0  # Checked for convergence

        # Looping through each sample in the dataset:
        for i in range(X.shape[0]):
            # Predicting output for current sample:
            y_hat = perceptron(W, X_bias[i])

            # For calculating the error (difference between actual and predicted output):
            e = Y[i] - y_hat

            # Updating weights if there is a misclassification (error != 0):
            if e != 0:
                W += eta * e * X_bias[i]  # Adjusts the weights based on error.
                errors += 1  # Increments the error count.

        # If no errors occurred, model has fully converged:
        if errors == 0:
            print(f"Converged after {epoch+1} epochs.")
            break

    # Returning the final weight vector
    return W

# Decision boundary plotting function:
# This function plots the decision boundary for the trained perceptron model:
# Parameters:
#   X: Input features
#   Y: Target labels
#   W: Weight vector
def plot_decision_boundary(X, Y, W):
    plt.figure(figsize=(8, 6))  # Setting plot size.

    # Plot positive and negative class samples in different colors:
    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], color="blue", label="Class +1")
    plt.scatter(X[Y == -1, 0], X[Y == -1, 1], color="red", label="Class -1")

    # Calculating slope (m) and intercept (b) for the decision boundary:
    slope = -W[1] / W[2]
    intercept = -W[0] / W[2]


    # Generates x values for the decision boundary lines:
    x_vals = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)
    colors = ["k", "g", "m", "c"]  # Colors for different boundaries

    # Corresponding y values for the decision boundary line:
    y_vals = slope * x_vals + intercept
    plt.plot(x_vals, y_vals, "k--", label="Decision Boundary")

    # For setting the plot labels and title:
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.legend()
    plt.title("Perceptron Decision Boundary")
    plt.grid(True)

    # Display:
    plt.show()

"""# Dataset 1"""

X, Y = make_classification(
        n_samples=146,
        n_features=6,
        n_informative=3,
        n_redundant=1,
        n_classes=2,
        flip_y=0.1,
        class_sep=1.4,
        n_clusters_per_class=1,
        random_state=14,
    )
Y[Y==0] = -1 # setted 0 labels to -1 for perceptron:
plt.plot(X[Y==-1, 0], X[Y==-1, 1], 'ro')
plt.plot(X[Y==1, 0], X[Y==1, 1], 'bo');

"""# Dataset 2"""

X, Y = make_classification(
      n_samples=146,
        n_features=6,
        n_informative=3,
        n_redundant=1,
        n_classes=2,
        n_clusters_per_class=1,
        random_state=6,

    )
Y[Y==0] = -1 # again setted 0 labels to -1 for perceptron:
plt.plot(X[Y==-1, 0], X[Y==-1, 1], 'ro')
plt.plot(X[Y==1, 0], X[Y==1, 1], 'bo');

"""# Decision Boundary

"""

# Training the perceptron with generated data:
W_trained = perceptron_train(X, Y, eta=1, epochs=100)

# Plotting the decision boundary using the trained weights:
plot_decision_boundary(X, Y, W_trained)

"""# Extra Datasets"""

# Extra Dataset 1:
X2, Y2 = make_classification(
    n_samples=20,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_classes=2,
    n_clusters_per_class=1,
    random_state=6,
)
Y2[Y2 == 0] = -1  # 0 labels to -1
W_trained2 = perceptron_train(X2, Y2, eta=1, epochs=100)
plot_decision_boundary(X2, Y2, W_trained2)

# Extra Dataset 2:
X3, Y3 = make_classification(
    n_samples=20,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_classes=2,
    n_clusters_per_class=1,
    random_state=14,
)
Y3[Y3 == 0] = -1  # Convert 0 labels to -1
W_trained3 = perceptron_train(X3, Y3, eta=1, epochs=100)
plot_decision_boundary(X3, Y3, W_trained3)